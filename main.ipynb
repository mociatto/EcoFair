{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EcoFair: Domain-Adaptive Green AI for Skin Cancer Detection\n",
    "\n",
    "## Abstract\n",
    "\n",
    "EcoFair introduces a novel **Green AI** pipeline for skin cancer detection that adapts to domain shifts while maintaining diagnostic accuracy and energy efficiency. The system combines three key innovations:\n",
    "\n",
    "1. **VFL Architecture**: Split computing paradigm using lightweight (Lite) and heavy (Heavy) models, enabling efficient edge deployment.\n",
    "\n",
    "2. **Neurosymbolic Scoring**: Embedding clinical domain knowledge (sun exposure risk based on body localization) into the model's decision-making process.\n",
    "\n",
    "3. **Resource-Aware Routing**: Adaptive switching between Safety-First optimization (for stable domains) and Budget-Constrained routing (for high-entropy domains) based on observed domain shift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (if not already cloned)\n",
    "!git clone https://github.com/mociatto/EcoFair.git\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the cloned repo to the Python path\n",
    "if os.path.exists('./EcoFair'):\n",
    "    sys.path.append('./EcoFair')\n",
    "elif os.path.exists('./src'):\n",
    "    # If running from project root\n",
    "    sys.path.append('.')\n",
    "\n",
    "# Import EcoFair modules\n",
    "from src import config, utils, data_loader, models, training, features, routing, fairness, visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set reproducibility\n",
    "utils.set_seed(config.RANDOM_STATE)\n",
    "\n",
    "print(f\"EcoFair v{config.VERSION} loaded successfully.\")\n",
    "print(f\"Using models: {config.SELECTED_LITE_MODEL} (Lite) and {config.SELECTED_HEAVY_MODEL} (Heavy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 1: The Benchmark (HAM10000)\n",
    "\n",
    "We first establish a baseline on the source domain (**Dermoscopy**). The HAM10000 dataset provides high-quality dermoscopic images with consistent imaging conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data Loading & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and align HAM10000 data\n",
    "print(\"Loading HAM10000 data...\")\n",
    "X_heavy, X_lite, meta_ham = data_loader.load_and_align_ham()\n",
    "print(f\"Loaded {len(meta_ham)} samples\")\n",
    "print(f\"Heavy features shape: {X_heavy.shape}\")\n",
    "print(f\"Lite features shape: {X_lite.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tabular features with Neurosymbolic Risk Scoring\n",
    "print(\"\\nPreparing tabular features...\")\n",
    "X_tab, scaler, sex_encoder, loc_encoder, risk_scaler = features.prepare_tabular_features(meta_ham)\n",
    "print(f\"Tabular features shape: {X_tab.shape}\")\n",
    "\n",
    "# Prepare labels\n",
    "y_ham, dx_to_idx = features.prepare_labels(meta_ham)\n",
    "print(f\"Labels shape: {y_ham.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data using stratified group K-fold\n",
    "y_labels_ham = np.argmax(y_ham, axis=1)\n",
    "splits = data_loader.get_stratified_split(meta_ham, y_labels_ham, n_splits=5)\n",
    "train_idx, test_idx = list(splits)[0]\n",
    "\n",
    "# Further split train into train/val\n",
    "meta_train = meta_ham.iloc[train_idx].reset_index(drop=True)\n",
    "y_train_labels = y_labels_ham[train_idx]\n",
    "splits_val = data_loader.get_stratified_split(meta_train, y_train_labels, n_splits=5)\n",
    "train_idx_final, val_idx = list(splits_val)[0]\n",
    "\n",
    "# Get absolute indices\n",
    "train_idx_abs = train_idx[train_idx_final]\n",
    "val_idx_abs = train_idx[val_idx]\n",
    "\n",
    "# Split features\n",
    "X_lite_train = X_lite[train_idx_abs]\n",
    "X_lite_val = X_lite[val_idx_abs]\n",
    "X_lite_test = X_lite[test_idx]\n",
    "\n",
    "X_heavy_train = X_heavy[train_idx_abs]\n",
    "X_heavy_val = X_heavy[val_idx_abs]\n",
    "X_heavy_test = X_heavy[test_idx]\n",
    "\n",
    "X_tab_train = X_tab[train_idx_abs]\n",
    "X_tab_val = X_tab[val_idx_abs]\n",
    "X_tab_test = X_tab[test_idx]\n",
    "\n",
    "y_train = y_ham[train_idx_abs]\n",
    "y_val = y_ham[val_idx_abs]\n",
    "y_test = y_ham[test_idx]\n",
    "\n",
    "meta_test = meta_ham.iloc[test_idx].reset_index(drop=True)\n",
    "meta_val = meta_train.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(y_train)}, Val: {len(y_val)}, Test: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build models\n",
    "print(\"Building VFL models...\")\n",
    "lite_adapter = models.build_image_adapter(feature_dim=X_lite.shape[1], embedding_dim=128)\n",
    "heavy_adapter = models.build_image_adapter(feature_dim=X_heavy.shape[1], embedding_dim=128)\n",
    "tab_client = models.build_tabular_client(input_dim=X_tab.shape[1], embedding_dim=128)\n",
    "server_head = models.build_server_head(input_dim=256, num_classes=len(config.CLASS_NAMES))\n",
    "\n",
    "lite_model = models.build_vfl_model(lite_adapter, tab_client, server_head)\n",
    "heavy_model = models.build_vfl_model(heavy_adapter, tab_client, server_head)\n",
    "\n",
    "print(\"Models built successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class weights\n",
    "class_weight_dict = training.get_class_weights(y_train)\n",
    "print(\"\\nClass weights:\")\n",
    "for i, class_name in enumerate(config.CLASS_NAMES):\n",
    "    print(f\"  {class_name}: {class_weight_dict[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Lite Model\n",
    "print(\"\\nTraining Lite Model...\")\n",
    "lite_history = training.compile_and_train(\n",
    "    lite_model, X_lite_train, X_tab_train, y_train,\n",
    "    X_lite_val, X_tab_val, y_val,\n",
    "    class_weight=class_weight_dict\n",
    ")\n",
    "print(\"Lite model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Heavy Model\n",
    "print(\"\\nTraining Heavy Model...\")\n",
    "heavy_history = training.compile_and_train(\n",
    "    heavy_model, X_heavy_train, X_tab_train, y_train,\n",
    "    X_heavy_val, X_tab_val, y_val,\n",
    "    class_weight=class_weight_dict\n",
    ")\n",
    "print(\"Heavy model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Safety-First Optimization & Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on validation set\n",
    "print(\"Generating predictions...\")\n",
    "lite_preds_val = lite_model.predict([X_lite_val, X_tab_val], batch_size=config.BATCH_SIZE, verbose=0)\n",
    "heavy_preds_val = heavy_model.predict([X_heavy_val, X_tab_val], batch_size=config.BATCH_SIZE, verbose=0)\n",
    "\n",
    "y_true_val = np.argmax(y_val, axis=1)\n",
    "\n",
    "# Calculate entropy and safe-danger gap\n",
    "entropy_val = routing.calculate_entropy(lite_preds_val)\n",
    "safe_indices = [config.CLASS_NAMES.index(c) for c in config.SAFE_CLASSES]\n",
    "danger_indices = [config.CLASS_NAMES.index(c) for c in config.DANGEROUS_CLASSES]\n",
    "prob_safe_val = lite_preds_val[:, safe_indices].sum(axis=1)\n",
    "prob_danger_val = lite_preds_val[:, danger_indices].sum(axis=1)\n",
    "safe_danger_gap_val = prob_safe_val - prob_danger_val\n",
    "\n",
    "# Calculate baseline accuracy\n",
    "heavy_baseline_acc = accuracy_score(y_true_val, np.argmax(heavy_preds_val, axis=1))\n",
    "print(f\"Heavy baseline accuracy: {heavy_baseline_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize thresholds using SafetyFirstOptimizer\n",
    "print(\"\\nOptimizing routing thresholds...\")\n",
    "optimizer = routing.SafetyFirstOptimizer(\n",
    "    lite_preds_val, heavy_preds_val, y_true_val,\n",
    "    entropy_val, safe_danger_gap_val, heavy_baseline_acc\n",
    ")\n",
    "\n",
    "optimal_config, all_results = optimizer.optimize()\n",
    "\n",
    "print(f\"\\nOptimal Configuration:\")\n",
    "print(f\"  Entropy Threshold: {optimal_config['entropy_t']:.2f}\")\n",
    "print(f\"  Gap Threshold: {optimal_config['gap_t']:.2f}\")\n",
    "print(f\"  Heavy Weight: {optimal_config['heavy_weight']:.2f}\")\n",
    "print(f\"  Accuracy: {optimal_config['accuracy']:.4f}\")\n",
    "print(f\"  Intervention Rate: {optimal_config['intervention_rate']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split PAD data for training and testing\n",
    "print(\"\\nSplitting PAD data...\")\n",
    "y_labels_pad = np.argmax(y_pad, axis=1)\n",
    "splits_pad = data_loader.get_stratified_split(meta_pad, y_labels_pad, n_splits=5)\n",
    "train_idx_pad, test_idx_pad = list(splits_pad)[0]\n",
    "\n",
    "# Further split train into train/val\n",
    "meta_train_pad = meta_pad.iloc[train_idx_pad].reset_index(drop=True)\n",
    "y_train_labels_pad = y_labels_pad[train_idx_pad]\n",
    "splits_val_pad = data_loader.get_stratified_split(meta_train_pad, y_train_labels_pad, n_splits=5)\n",
    "train_idx_final_pad, val_idx_pad = list(splits_val_pad)[0]\n",
    "\n",
    "# Get absolute indices\n",
    "train_idx_abs_pad = train_idx_pad[train_idx_final_pad]\n",
    "val_idx_abs_pad = train_idx_pad[val_idx_pad]\n",
    "\n",
    "# Split features\n",
    "X_lite_train_pad = X_lite_pad[train_idx_abs_pad]\n",
    "X_lite_val_pad = X_lite_pad[val_idx_abs_pad]\n",
    "X_lite_test_pad = X_lite_pad[test_idx_pad]\n",
    "\n",
    "X_heavy_train_pad = X_heavy_pad[train_idx_abs_pad]\n",
    "X_heavy_val_pad = X_heavy_pad[val_idx_abs_pad]\n",
    "X_heavy_test_pad = X_heavy_pad[test_idx_pad]\n",
    "\n",
    "X_tab_train_pad = X_tab_pad[train_idx_abs_pad]\n",
    "X_tab_val_pad = X_tab_pad[val_idx_abs_pad]\n",
    "X_tab_test_pad = X_tab_pad[test_idx_pad]\n",
    "\n",
    "y_train_pad = y_pad[train_idx_abs_pad]\n",
    "y_val_pad = y_pad[val_idx_abs_pad]\n",
    "y_test_pad = y_pad[test_idx_pad]\n",
    "\n",
    "meta_test_pad = meta_pad.iloc[test_idx_pad].reset_index(drop=True)\n",
    "meta_val_pad = meta_train_pad.iloc[val_idx_pad].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(y_train_pad)}, Val: {len(y_val_pad)}, Test: {len(y_test_pad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safety check: Ensure models are built",
    "if 'lite_model_pad' not in globals() or 'heavy_model_pad' not in globals():",
    "    raise NameError(\"PAD models not found! Please execute the model building cell (## 2.2) first.\")",
    "",
    "# Train PAD models",
    "# Note: For demo purposes, we use fewer epochs. In production, use config.EPOCHS (30)",
    "print(\"\\nTraining PAD Lite Model...\")",
    "class_weight_dict_pad = training.get_class_weights(y_train_pad, class_names=PAD_CLASS_NAMES)",
    "",
    "lite_history_pad = training.compile_and_train(",
    "    lite_model_pad, X_lite_train_pad, X_tab_train_pad, y_train_pad,",
    "    X_lite_val_pad, X_tab_val_pad, y_val_pad,",
    "    class_weight=class_weight_dict_pad",
    ")",
    "print(\"PAD Lite model training complete.\")",
    "",
    "print(\"\\nTraining PAD Heavy Model...\")",
    "heavy_history_pad = training.compile_and_train(",
    "    heavy_model_pad, X_heavy_train_pad, X_tab_train_pad, y_train_pad,",
    "    X_heavy_val_pad, X_tab_val_pad, y_val_pad,",
    "    class_weight=class_weight_dict_pad",
    ")",
    "print(\"PAD Heavy model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply optimized routing on test set\n",
    "print(\"\\nApplying routing on test set...\")\n",
    "lite_preds_test = lite_model.predict([X_lite_test, X_tab_test], batch_size=config.BATCH_SIZE, verbose=0)\n",
    "heavy_preds_test = heavy_model.predict([X_heavy_test, X_tab_test], batch_size=config.BATCH_SIZE, verbose=0)\n",
    "\n",
    "final_preds_ham, route_mask_ham = routing.apply_threshold_routing(\n",
    "    lite_preds_test, heavy_preds_test,\n",
    "    entropy_threshold=optimal_config['entropy_t'],\n",
    "    gap_threshold=optimal_config['gap_t'],\n",
    "    heavy_weight=optimal_config['heavy_weight']\n",
    ")\n",
    "\n",
    "y_true_test = np.argmax(y_test, axis=1)\n",
    "y_pred_ham = np.argmax(final_preds_ham, axis=1)\n",
    "\n",
    "acc_ham = accuracy_score(y_true_test, y_pred_ham)\n",
    "acc_lite = accuracy_score(y_true_test, np.argmax(lite_preds_test, axis=1))\n",
    "acc_heavy = accuracy_score(y_true_test, np.argmax(heavy_preds_test, axis=1))\n",
    "\n",
    "print(f\"\\nTest Set Results:\")\n",
    "print(f\"  Lite Accuracy: {acc_lite:.4f}\")\n",
    "print(f\"  Heavy Accuracy: {acc_heavy:.4f}\")\n",
    "print(f\"  EcoFair Accuracy: {acc_ham:.4f}\")\n",
    "print(f\"  Routing Rate: {route_mask_ham.sum() / len(route_mask_ham) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Comparison\n",
    "fig_cm = visualization.plot_confusion_matrix_comparison(\n",
    "    y_true_test, lite_preds_test, heavy_preds_test, final_preds_ham\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value-Added Analysis\n",
    "fig_va = visualization.plot_value_added_bars(\n",
    "    y_true_test, lite_preds_test, heavy_preds_test, final_preds_ham,\n",
    "    route_mask=route_mask_ham\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 2: Domain Generalization (PAD-UFES-20)\n",
    "\n",
    "We now test robustness on the target domain (**Clinical Images**), where significant domain shift occurs. Clinical images have different lighting, angles, and backgrounds compared to dermoscopic images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data Loading & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PAD-UFES-20 data\n",
    "# Note: Update path to your PAD metadata location\n",
    "PAD_METADATA_PATH = '/kaggle/input/skin-cancer/metadata.csv'  # Update this path\n",
    "\n",
    "print(\"Loading PAD-UFES-20 data...\")\n",
    "X_heavy_pad, X_lite_pad, meta_pad = data_loader.load_and_align_pad(PAD_METADATA_PATH)\n",
    "print(f\"Loaded {len(meta_pad)} samples\")\n",
    "print(f\"Heavy features shape: {X_heavy_pad.shape}\")\n",
    "print(f\"Lite features shape: {X_lite_pad.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tabular features (automatically handles column mapping)\n",
    "print(\"\\nPreparing tabular features...\")\n",
    "X_tab_pad, scaler_pad, sex_encoder_pad, loc_encoder_pad, risk_scaler_pad = features.prepare_tabular_features(meta_pad)\n",
    "print(f\"Tabular features shape: {X_tab_pad.shape}\")\n",
    "\n",
    "# Prepare labels (using PAD class names)\n",
    "# Note: Update PAD_CLASS_NAMES in config if needed\n",
    "PAD_CLASS_NAMES = ['bcc', 'scc', 'mel', 'ack', 'nev', 'sek']  # PAD-specific classes\n",
    "y_pad, dx_to_idx_pad = features.prepare_labels(meta_pad, class_names=PAD_CLASS_NAMES)\n",
    "print(f\"Labels shape: {y_pad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Budget-Constrained Routing\n",
    "\n",
    "**Key Insight**: Due to high entropy in clinical images, standard threshold-based routing fails. We switch to **Budget-Constrained Routing** (Top-35% Uncertainty), which guarantees a fixed energy budget while routing the most uncertain samples to the heavy model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Model Building & Training\n",
    "\n",
    "**Important**: PAD-UFES-20 requires separate models due to different tabular feature dimensions and class structure. We build and train PAD-specific models here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build PAD-specific models (required due to different tabular feature dimensions)",
    "# PAD has different number of localization categories, so tabular input dimension differs",
    "print(\"Building PAD-specific models with correct dimensions...\")",
    "",
    "# Build PAD models with correct tabular input dimension",
    "lite_adapter_pad = models.build_image_adapter(feature_dim=X_lite_pad.shape[1], embedding_dim=128)",
    "heavy_adapter_pad = models.build_image_adapter(feature_dim=X_heavy_pad.shape[1], embedding_dim=128)",
    "tab_client_pad = models.build_tabular_client(input_dim=X_tab_pad.shape[1], embedding_dim=128)",
    "server_head_pad = models.build_server_head(input_dim=256, num_classes=len(PAD_CLASS_NAMES))",
    "",
    "lite_model_pad = models.build_vfl_model(lite_adapter_pad, tab_client_pad, server_head_pad)",
    "heavy_model_pad = models.build_vfl_model(heavy_adapter_pad, tab_client_pad, server_head_pad)",
    "",
    "print(\"PAD models built successfully.\")",
    "print(f\"Tabular input dimension: {X_tab_pad.shape[1]} (PAD-specific)\")",
    "print(f\"Number of classes: {len(PAD_CLASS_NAMES)} (PAD-specific)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split PAD data for training and testing\n",
    "print(\"\\nSplitting PAD data...\")\n",
    "y_labels_pad = np.argmax(y_pad, axis=1)\n",
    "splits_pad = data_loader.get_stratified_split(meta_pad, y_labels_pad, n_splits=5)\n",
    "train_idx_pad, test_idx_pad = list(splits_pad)[0]\n",
    "\n",
    "# Further split train into train/val\n",
    "meta_train_pad = meta_pad.iloc[train_idx_pad].reset_index(drop=True)\n",
    "y_train_labels_pad = y_labels_pad[train_idx_pad]\n",
    "splits_val_pad = data_loader.get_stratified_split(meta_train_pad, y_train_labels_pad, n_splits=5)\n",
    "train_idx_final_pad, val_idx_pad = list(splits_val_pad)[0]\n",
    "\n",
    "# Get absolute indices\n",
    "train_idx_abs_pad = train_idx_pad[train_idx_final_pad]\n",
    "val_idx_abs_pad = train_idx_pad[val_idx_pad]\n",
    "\n",
    "# Split features\n",
    "X_lite_train_pad = X_lite_pad[train_idx_abs_pad]\n",
    "X_lite_val_pad = X_lite_pad[val_idx_abs_pad]\n",
    "X_lite_test_pad = X_lite_pad[test_idx_pad]\n",
    "\n",
    "X_heavy_train_pad = X_heavy_pad[train_idx_abs_pad]\n",
    "X_heavy_val_pad = X_heavy_pad[val_idx_abs_pad]\n",
    "X_heavy_test_pad = X_heavy_pad[test_idx_pad]\n",
    "\n",
    "X_tab_train_pad = X_tab_pad[train_idx_abs_pad]\n",
    "X_tab_val_pad = X_tab_pad[val_idx_abs_pad]\n",
    "X_tab_test_pad = X_tab_pad[test_idx_pad]\n",
    "\n",
    "y_train_pad = y_pad[train_idx_abs_pad]\n",
    "y_val_pad = y_pad[val_idx_abs_pad]\n",
    "y_test_pad = y_pad[test_idx_pad]\n",
    "\n",
    "meta_test_pad = meta_pad.iloc[test_idx_pad].reset_index(drop=True)\n",
    "meta_val_pad = meta_train_pad.iloc[val_idx_pad].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(y_train_pad)}, Val: {len(y_val_pad)}, Test: {len(y_test_pad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PAD models\n",
    "# Note: For demo purposes, we use fewer epochs. In production, use config.EPOCHS (30)\n",
    "print(\"\\nTraining PAD Lite Model...\")\n",
    "class_weight_dict_pad = training.get_class_weights(y_train_pad, class_names=PAD_CLASS_NAMES)\n",
    "\n",
    "lite_history_pad = training.compile_and_train(\n",
    "    lite_model_pad, X_lite_train_pad, X_tab_train_pad, y_train_pad,\n",
    "    X_lite_val_pad, X_tab_val_pad, y_val_pad,\n",
    "    class_weight=class_weight_dict_pad\n",
    ")\n",
    "print(\"PAD Lite model training complete.\")\n",
    "\n",
    "print(\"\\nTraining PAD Heavy Model...\")\n",
    "heavy_history_pad = training.compile_and_train(\n",
    "    heavy_model_pad, X_heavy_train_pad, X_tab_train_pad, y_train_pad,\n",
    "    X_heavy_val_pad, X_tab_val_pad, y_val_pad,\n",
    "    class_weight=class_weight_dict_pad\n",
    ")\n",
    "print(\"PAD Heavy model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions using PAD-specific models\n",
    "print(\"\\nGenerating predictions on PAD test set...\")\n",
    "lite_preds_pad = lite_model_pad.predict([X_lite_test_pad, X_tab_test_pad], batch_size=config.BATCH_SIZE, verbose=0)\n",
    "heavy_preds_pad = heavy_model_pad.predict([X_heavy_test_pad, X_tab_test_pad], batch_size=config.BATCH_SIZE, verbose=0)\n",
    "\n",
    "print(f\"Test set size: {len(y_test_pad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Budget-Constrained Routing (35% budget)\n",
    "print(\"\\nApplying Budget-Constrained Routing (35% budget)...\")\n",
    "\n",
    "# Use PAD class names for routing\n",
    "PAD_SAFE_CLASSES = ['nev', 'sek']\n",
    "PAD_DANGEROUS_CLASSES = ['bcc', 'scc', 'mel', 'ack']\n",
    "\n",
    "final_preds_pad, route_mask_pad, confusion_score_pad = routing.apply_budget_routing(\n",
    "    lite_preds_pad, heavy_preds_pad,\n",
    "    budget=0.35,\n",
    "    heavy_weight=0.5,\n",
    "    class_names=PAD_CLASS_NAMES,\n",
    "    safe_classes=PAD_SAFE_CLASSES,\n",
    "    danger_classes=PAD_DANGEROUS_CLASSES\n",
    ")\n",
    "\n",
    "y_true_pad = np.argmax(y_test_pad, axis=1)\n",
    "y_pred_pad = np.argmax(final_preds_pad, axis=1)\n",
    "\n",
    "acc_pad = accuracy_score(y_true_pad, y_pred_pad)\n",
    "acc_lite_pad = accuracy_score(y_true_pad, np.argmax(lite_preds_pad, axis=1))\n",
    "acc_heavy_pad = accuracy_score(y_true_pad, np.argmax(heavy_preds_pad, axis=1))\n",
    "\n",
    "print(f\"\\nPAD Test Set Results:\")\n",
    "print(f\"  Lite Accuracy: {acc_lite_pad:.4f}\")\n",
    "print(f\"  Heavy Accuracy: {acc_heavy_pad:.4f}\")\n",
    "print(f\"  EcoFair Accuracy: {acc_pad:.4f}\")\n",
    "print(f\"  Routing Rate: {route_mask_pad.sum() / len(route_mask_pad) * 100:.2f}% (Fixed at 35%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Energy Efficiency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load energy statistics\n",
    "joules_per_lite = utils.load_energy_stats(config.SELECTED_LITE_MODEL, is_heavy=False)\n",
    "joules_per_heavy = utils.load_energy_stats(config.SELECTED_HEAVY_MODEL, is_heavy=True)\n",
    "\n",
    "# Use defaults if not found\n",
    "if joules_per_lite is None:\n",
    "    joules_per_lite = 1.0\n",
    "if joules_per_heavy is None:\n",
    "    joules_per_heavy = 2.5\n",
    "\n",
    "print(f\"Energy per sample:\")\n",
    "print(f\"  Lite: {joules_per_lite:.6f} J\")\n",
    "print(f\"  Heavy: {joules_per_heavy:.6f} J\")\n",
    "\n",
    "# Calculate routing rate\n",
    "routing_rate_pad = route_mask_pad.sum() / len(route_mask_pad)\n",
    "\n",
    "# Plot battery decay\n",
    "fig_battery = visualization.plot_battery_decay(\n",
    "    lite_joules=joules_per_lite,\n",
    "    heavy_joules=joules_per_heavy,\n",
    "    routing_rate=routing_rate_pad,\n",
    "    capacity_joules=10000\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 3: Fairness Audit\n",
    "\n",
    "We empirically audit the model's fairness across **Age** and **Sex** subgroups to ensure equitable performance across demographic groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 HAM10000 Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fairness report for HAM10000\n",
    "print(\"HAM10000 Fairness Report:\")\n",
    "ham_fairness = fairness.generate_fairness_report(\n",
    "    y_true_test, y_pred_ham, meta_test\n",
    ")\n",
    "print(\"\\n\")\n",
    "display(ham_fairness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 PAD-UFES-20 Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fairness report for PAD-UFES-20\n",
    "print(\"PAD-UFES-20 Fairness Report:\")\n",
    "pad_fairness = fairness.generate_fairness_report(\n",
    "    y_true_pad, y_pred_pad, meta_test_pad,\n",
    "    class_names=PAD_CLASS_NAMES\n",
    ")\n",
    "print(\"\\n\")\n",
    "display(pad_fairness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "EcoFair demonstrates:\n",
    "\n",
    "1. **Effective Domain Adaptation**: Maintains performance across dermoscopic (HAM10000) and clinical (PAD-UFES-20) domains.\n",
    "\n",
    "2. **Energy Efficiency**: Budget-constrained routing enables significant energy savings while preserving diagnostic accuracy.\n",
    "\n",
    "3. **Fairness**: Systematic evaluation across demographic subgroups ensures equitable performance.\n",
    "\n",
    "The modular architecture enables easy extension to new domains and routing strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}